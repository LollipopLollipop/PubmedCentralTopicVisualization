{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Latent Dirichlet allocation (LDA) is a widely discussed topic modeling algorithm. According to David M. Blei's paper \"Probabilistic Topic Models\", in the domain of LDA, a topic is defined as \"a distribution over a fixed vocabulary\" and the key idea behind LDA is that documents exhibit multiple topics. LDA is a statistical topic model of document collections. \n",
    "\n",
    "My experiments for this project would mostly concentrate on LDA algorithm. \n",
    "\n",
    "Computing library used: sklearn (interoperate with NumPy and SciPy)\n",
    "\n",
    "In the newest version of sklearn, class \"LatentDirichletAllocation\" is added, which performs Latent Dirichlet Allocation with online variational Bayes algorithm on given data matrix.\n",
    "\n",
    "Below is source code for my first experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "n_distributions = 100 #number of distributions\n",
    "n_top_words = 20 #number of terms extracted in each distribution \n",
    "\n",
    "#helper function\n",
    "def print_feature_names(feature_names):\n",
    "    for f in feature_names:\n",
    "        print(f)\n",
    "\n",
    "#helper function to find term index in given vocabulary        \n",
    "def find_term_idx(term,feature_names):\n",
    "    for idx, f in enumerate(feature_names):\n",
    "        if(f==term):\n",
    "            return idx\n",
    "    \n",
    "#helper function to print most commonly co-occurred terms (with score) in each distribution    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([str(topic[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#helper function to find the distribution in which the target term is mostly \"related\" \n",
    "#and get the top terms in that distribution\n",
    "def find_most_relevant(model, feature_names, n_top_words, target_term_idx):\n",
    "    max_score = 0.0\n",
    "    max_idx = -1\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if(topic[target_term_idx]>max_score):\n",
    "            max_score = topic[target_term_idx]\n",
    "            max_idx = topic_idx\n",
    "            \n",
    "    max_topic = model.components_[max_idx]\n",
    "    #relevant_terms = \" \".join([feature_names[i] for i in max_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    #print(\"Most Relevant Topic #%d:%s\" % (max_idx,relevant_terms))\n",
    "    relevant_terms = set()\n",
    "    for i in max_topic.argsort()[:-n_top_words - 1:-1]:\n",
    "        relevant_terms.add(feature_names[i])\n",
    "    return relevant_terms\n",
    "\n",
    "#helper function to convert a set object to list that can be directly json encoded   \n",
    "def set_convert_to_json_list(terms_set):\n",
    "    json_term_list = list()\n",
    "    for term in terms_set:\n",
    "        json_term_list.append({\"name\":term})\n",
    "    return json_term_list\n",
    "\n",
    "#helper function to convert a dict object to list that can be directly json encoded\n",
    "def dict_convert_to_json_list(terms_dict):\n",
    "    json_term_list = list()\n",
    "    for key in terms_dict:\n",
    "        json_term_list.append({\"name\":key, \"children\":set_convert_to_json_list(terms_dict[key])})\n",
    "    return json_term_list\n",
    "    \n",
    "\n",
    "#exit if no user term is provided    \n",
    "if(len(sys.argv)!=2):\n",
    "    print (\"Invalid input...exit\")\n",
    "    exit()\n",
    "    \n",
    "user_term = sys.argv[1]\n",
    "\n",
    "#load the PMC dataset\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "\n",
    "data_samples = list()\n",
    "for folder, dirs, files in os.walk('test/'):\n",
    "    for f in files:\n",
    "        if f.endswith('.nxml'):\n",
    "            tree = ET.parse(os.path.join(folder,f))\n",
    "            root = tree.getroot()\n",
    "            data_samples.append(' '.join(root.itertext())) #each document is represented by text content from that xml file\n",
    "            \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"%d lines read\" % len(data_samples))\n",
    "\n",
    "#use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "#max_df=0.95: when building the vocabulary ignore terms that have a document frequency strictly > 95% of documents\n",
    "#min_df=2: when building the vocabulary ignore terms that have a document frequency strictly < 2 \n",
    "#stop_words='english': a built-in stop word list for English is used\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features\")\n",
    "#learning_method='online': method used to update _component\n",
    "#in general, if the data size is large, the online update will be much faster than the batch update\n",
    "#'online': Online variational Bayes method.\n",
    "\n",
    "#learning_offset=50.:learning rate, a (positive) parameter that downweights early iterations in online learning\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "#for debugging purposes\n",
    "#print(\"\\nFeature Names in LDA model:\")\n",
    "#print_feature_names(tf_feature_names)\n",
    "#print(\"\\nTopics in LDA model:\")\n",
    "#print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "#to extract terms(topics) that most commonly co-occurred with the user provided term (categorized in 2 levels)\n",
    "\n",
    "#set containing terms already shown in previous levels\n",
    "shown_terms = set()\n",
    "shown_terms.add(user_term)\n",
    "user_term_idx = find_term_idx(user_term,tf_feature_names)\n",
    "#l1_relevant_terms meaning the level 1 topics that are mostly commonly co-occurred\n",
    "l1_relevant_terms = find_most_relevant(lda, tf_feature_names, n_top_words, user_term_idx)\n",
    "l1_relevant_terms = l1_relevant_terms.difference(shown_terms)\n",
    "#update shown_terms set for l2_relevant_terms\n",
    "shown_terms = shown_terms.union(l1_relevant_terms)\n",
    "l2_relevant_terms = dict()\n",
    "#level 2 topics are grouped by level 1 topics as keys\n",
    "for term in l1_relevant_terms:\n",
    "    term_idx = find_term_idx(term,tf_feature_names)\n",
    "    l2_relevant_terms[term] = find_most_relevant(lda, tf_feature_names, n_top_words, term_idx)\n",
    "    l2_relevant_terms[term] = l2_relevant_terms[term].difference(shown_terms)\n",
    "    \n",
    "#convert obj to json file\n",
    "model = dict()\n",
    "model[\"name\"] = user_term\n",
    "model[\"children\"] = dict_convert_to_json_list(l2_relevant_terms)\n",
    "\n",
    "\n",
    "output = open('flare.json','w')\n",
    "output.write(json.dumps(model,indent=1)) # python will convert \\n to os.linesep\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As can be seen from the code, in the first experiment:\n",
    "1. documents are represented by bag of words model\n",
    "2. text corpus is preprocessed using token counts metric after removing English stop words, ignoring terms occuring in >95% documents and only focusing on terms occuring in at least 2 documents\n",
    "3. LDA model is built upon feature matrix with top n distribution (in LDA language, a distribution is a topic) extracted\n",
    "4. for each target term, terms most commonly co-occurred with it are identified as those that are most frequent in the distribution in which the target term is most frequent (as compared to in other distributions in the same corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/v1_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model visualization, n_distributions = 10, user provided term as \"chemical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/v1_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model visualization, n_distributions = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature extraction (preprocessing) stage, experiment 1 uses tf (token counts). To balance the weights of terms with different frequencies, experiment 2 hence extracts the tfidf features from the documents collection. Tfidf incorporates an inverse document frequency factor which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. Everything else in experiment 1 remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "n_topics = 10 #number of distributions\n",
    "n_top_words = 20 #number of terms extracted in each distribution \n",
    "\n",
    "#helper function\n",
    "def print_feature_names(feature_names):\n",
    "    for f in feature_names:\n",
    "        print(f)\n",
    "\n",
    "#helper function to find term index in given vocabulary        \n",
    "def find_term_idx(term,feature_names):\n",
    "    for idx, f in enumerate(feature_names):\n",
    "        if(f==term):\n",
    "            return idx\n",
    "    \n",
    "#helper function to print most commonly co-occurred terms (with score) in each distribution    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([str(topic[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#helper function to find the distribution in which the target term is mostly \"related\" \n",
    "#and get the top terms in that distribution\n",
    "def find_most_relevant(model, feature_names, n_top_words, target_term_idx):\n",
    "    max_score = 0.0\n",
    "    max_idx = -1\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if(topic[target_term_idx]>max_score):\n",
    "            max_score = topic[target_term_idx]\n",
    "            max_idx = topic_idx\n",
    "            \n",
    "    max_topic = model.components_[max_idx]\n",
    "    #relevant_terms = \" \".join([feature_names[i] for i in max_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    #print(\"Most Relevant Topic #%d:%s\" % (max_idx,relevant_terms))\n",
    "    relevant_terms = set()\n",
    "    for i in max_topic.argsort()[:-n_top_words - 1:-1]:\n",
    "        relevant_terms.add(feature_names[i])\n",
    "    return relevant_terms\n",
    "\n",
    "#helper function to convert a set object to list that can be directly json encoded   \n",
    "def set_convert_to_json_list(terms_set):\n",
    "    json_term_list = list()\n",
    "    for term in terms_set:\n",
    "        json_term_list.append({\"name\":term})\n",
    "    return json_term_list\n",
    "\n",
    "#helper function to convert a dict object to list that can be directly json encoded\n",
    "def dict_convert_to_json_list(terms_dict):\n",
    "    json_term_list = list()\n",
    "    for key in terms_dict:\n",
    "        json_term_list.append({\"name\":key, \"children\":set_convert_to_json_list(terms_dict[key])})\n",
    "    return json_term_list\n",
    "    \n",
    "\n",
    "#exit if no user term is provided    \n",
    "if(len(sys.argv)!=2):\n",
    "    print (\"Invalid input...exit\")\n",
    "    exit()\n",
    "    \n",
    "user_term = sys.argv[1]\n",
    "\n",
    "#load the PMC dataset\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "\n",
    "data_samples = list()\n",
    "for folder, dirs, files in os.walk('../data/'):\n",
    "    for f in files:\n",
    "        if f.endswith('.nxml'):\n",
    "            tree = ET.parse(os.path.join(folder,f))\n",
    "            root = tree.getroot()\n",
    "            data_samples.append(' '.join(root.itertext())) #each document is represented by text content from that xml file\n",
    "            \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"%d lines read\" % len(data_samples))\n",
    "\n",
    "#use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tfidf features for LDA...\")\n",
    "#max_df=0.95: when building the vocabulary ignore terms that have a document frequency strictly > 95% of documents\n",
    "#min_df=2: when building the vocabulary ignore terms that have a document frequency strictly < 2 \n",
    "#stop_words='english': a built-in stop word list for English is used\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tfidf features\")\n",
    "#learning_method='online': method used to update _component\n",
    "#in general, if the data size is large, the online update will be much faster than the batch update\n",
    "#'online': Online variational Bayes method.\n",
    "\n",
    "#learning_offset=50.:learning rate, a (positive) parameter that downweights early iterations in online learning\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#for debugging purposes\n",
    "#print(\"\\nFeature Names in LDA model:\")\n",
    "#print_feature_names(tf_feature_names)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "#to extract terms(topics) that most commonly co-occurred with the user provided term (categorized in 2 levels)\n",
    "\n",
    "#set containing terms already shown in previous levels\n",
    "shown_terms = set()\n",
    "shown_terms.add(user_term)\n",
    "user_term_idx = find_term_idx(user_term,tf_feature_names)\n",
    "#l1_relevant_terms meaning the level 1 topics that are mostly commonly co-occurred\n",
    "l1_relevant_terms = find_most_relevant(lda, tf_feature_names, n_top_words, user_term_idx)\n",
    "l1_relevant_terms = l1_relevant_terms.difference(shown_terms)\n",
    "#update shown_terms set for l2_relevant_terms\n",
    "shown_terms = shown_terms.union(l1_relevant_terms)\n",
    "l2_relevant_terms = dict()\n",
    "#level 2 topics are grouped by level 1 topics as keys\n",
    "for term in l1_relevant_terms:\n",
    "    term_idx = find_term_idx(term,tf_feature_names)\n",
    "    l2_relevant_terms[term] = find_most_relevant(lda, tf_feature_names, n_top_words, term_idx)\n",
    "    l2_relevant_terms[term] = l2_relevant_terms[term].difference(shown_terms)\n",
    "    \n",
    "#convert obj to json file\n",
    "model = dict()\n",
    "model[\"name\"] = user_term\n",
    "model[\"children\"] = dict_convert_to_json_list(l2_relevant_terms)\n",
    "\n",
    "\n",
    "output = open('../output/flare_v2_10.json','w')\n",
    "output.write(json.dumps(model,indent=1)) # python will convert \\n to os.linesep\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/v2_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model visualization, n_distributions = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tthe above visualization indicates that all level 2 ditributions overlap with the level 1 distribution (the distrbution extracted for the user provided term). Based on this observation, I update the code to ignore level 1 distribution when extracting level 2 distributions in experiment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "n_topics = 10 #number of distributions\n",
    "n_top_words = 20 #number of terms extracted in each distribution \n",
    "\n",
    "#helper function\n",
    "def print_feature_names(feature_names):\n",
    "    for f in feature_names:\n",
    "        print(f)\n",
    "\n",
    "#helper function to find term index in given vocabulary        \n",
    "def find_term_idx(term,feature_names):\n",
    "    for idx, f in enumerate(feature_names):\n",
    "        if(f==term):\n",
    "            return idx\n",
    "    \n",
    "#helper function to print most commonly co-occurred terms (with score) in each distribution    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([str(topic[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#helper function to find the distribution in which the target term is mostly \"related\" \n",
    "#and get the top terms in that distribution\n",
    "def find_most_relevant(model, target_term_idx, ignore_idx):\n",
    "    max_score = 0.0\n",
    "    max_idx = -1\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if(topic[target_term_idx]>max_score and topic_idx != ignore_idx):\n",
    "            max_score = topic[target_term_idx]\n",
    "            max_idx = topic_idx\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def get_top_terms(model, feature_names, n_top_words, topic_idx):\n",
    "    max_topic = model.components_[topic_idx]\n",
    "    #relevant_terms = \" \".join([feature_names[i] for i in max_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    #print(\"Most Relevant Topic #%d:%s\" % (max_idx,relevant_terms))\n",
    "    relevant_terms = set()\n",
    "    for i in max_topic.argsort()[:-n_top_words - 1:-1]:\n",
    "        relevant_terms.add(feature_names[i])\n",
    "    return relevant_terms\n",
    "    \n",
    "#helper function to convert a set object to list that can be directly json encoded   \n",
    "def set_convert_to_json_list(terms_set):\n",
    "    json_term_list = list()\n",
    "    for term in terms_set:\n",
    "        json_term_list.append({\"name\":term})\n",
    "    return json_term_list\n",
    "\n",
    "#helper function to convert a dict object to list that can be directly json encoded\n",
    "def dict_convert_to_json_list(terms_dict):\n",
    "    json_term_list = list()\n",
    "    for key in terms_dict:\n",
    "        json_term_list.append({\"name\":key, \"children\":set_convert_to_json_list(terms_dict[key])})\n",
    "    return json_term_list\n",
    "    \n",
    "\n",
    "#exit if no user term is provided    \n",
    "if(len(sys.argv)!=2):\n",
    "    print (\"Invalid input...exit\")\n",
    "    exit()\n",
    "    \n",
    "user_term = sys.argv[1]\n",
    "\n",
    "#load the PMC dataset\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "\n",
    "data_samples = list()\n",
    "for folder, dirs, files in os.walk('../data/'):\n",
    "    for f in files:\n",
    "        if f.endswith('.nxml'):\n",
    "            tree = ET.parse(os.path.join(folder,f))\n",
    "            root = tree.getroot()\n",
    "            data_samples.append(' '.join(root.itertext())) #each document is represented by text content from that xml file\n",
    "            \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"%d lines read\" % len(data_samples))\n",
    "\n",
    "#use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tfidf features for LDA...\")\n",
    "#max_df=0.95: when building the vocabulary ignore terms that have a document frequency strictly > 95% of documents\n",
    "#min_df=2: when building the vocabulary ignore terms that have a document frequency strictly < 2 \n",
    "#stop_words='english': a built-in stop word list for English is used\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tfidf features\")\n",
    "#learning_method='online': method used to update _component\n",
    "#in general, if the data size is large, the online update will be much faster than the batch update\n",
    "#'online': Online variational Bayes method.\n",
    "\n",
    "#learning_offset=50.:learning rate, a (positive) parameter that downweights early iterations in online learning\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#for debugging purposes\n",
    "#print(\"\\nFeature Names in LDA model:\")\n",
    "#print_feature_names(tf_feature_names)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "#to extract terms(topics) that most commonly co-occurred with the user provided term (categorized in 2 levels)\n",
    "\n",
    "#set containing terms already shown in previous levels\n",
    "shown_terms = set()\n",
    "shown_terms.add(user_term)\n",
    "user_term_idx = find_term_idx(user_term,tf_feature_names)\n",
    "#l1_relevant_terms meaning the level 1 topics that are mostly commonly co-occurred\n",
    "l1_relevant_distribution_idx = find_most_relevant(lda, user_term_idx, -1)\n",
    "l1_relevant_terms = get_top_terms(lda, tf_feature_names, n_top_words, l1_relevant_distribution_idx)\n",
    "l1_relevant_terms = l1_relevant_terms.difference(shown_terms)\n",
    "#update shown_terms set for l2_relevant_terms\n",
    "shown_terms = shown_terms.union(l1_relevant_terms)\n",
    "l2_relevant_terms = dict()\n",
    "#level 2 topics are grouped by level 1 topics as keys\n",
    "for term in l1_relevant_terms:\n",
    "    term_idx = find_term_idx(term,tf_feature_names)\n",
    "    tmp_distribution_idx = find_most_relevant(lda, term_idx, l1_relevant_distribution_idx)\n",
    "    l2_relevant_terms[term] = get_top_terms(lda, tf_feature_names, n_top_words, tmp_distribution_idx)\n",
    "    l2_relevant_terms[term] = l2_relevant_terms[term].difference(shown_terms)\n",
    "    \n",
    "#convert obj to json file\n",
    "model = dict()\n",
    "model[\"name\"] = user_term\n",
    "model[\"children\"] = dict_convert_to_json_list(l2_relevant_terms)\n",
    "\n",
    "\n",
    "output = open('../output/flare_v3_10.json','w')\n",
    "output.write(json.dumps(model,indent=1)) # python will convert \\n to os.linesep\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/v3_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model visualization, n_distributions = 10\n",
    "\n",
    "Using tfidf as compared to tf, it is observed that the model is generally improved. Terms with only numbers (such as 11, 12) are gone in the new model. Terms with only 1-3 characters that are more likely to be abbreviations (such as et, cu) are less frequent in the new model. In each level, more \"meaningful\" terms are extracted while the overlapping between each pair of them is less. For example, \"cell\" and \"cells\" are no longer in level 1, at least simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing research online, I notice that tfidf preprocessing is also widely used with NMF. \n",
    "Non-negative matrix factorization (NMF) is another popular algorithm used in topic modeling field. \n",
    "I perform the 4th experiment using NMF as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "n_topics = 10 #number of distributions\n",
    "n_top_words = 20 #number of terms extracted in each distribution \n",
    "\n",
    "#helper function\n",
    "def print_feature_names(feature_names):\n",
    "    for f in feature_names:\n",
    "        print(f)\n",
    "\n",
    "#helper function to find term index in given vocabulary        \n",
    "def find_term_idx(term,feature_names):\n",
    "    for idx, f in enumerate(feature_names):\n",
    "        if(f==term):\n",
    "            return idx\n",
    "    \n",
    "#helper function to print most commonly co-occurred terms (with score) in each distribution    \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([str(topic[i])\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#helper function to find the distribution in which the target term is mostly \"related\" \n",
    "#and get the top terms in that distribution\n",
    "def find_most_relevant(model, target_term_idx, ignore_idx):\n",
    "    max_score = 0.0\n",
    "    max_idx = -1\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if(topic[target_term_idx]>max_score and topic_idx != ignore_idx):\n",
    "            max_score = topic[target_term_idx]\n",
    "            max_idx = topic_idx\n",
    "    \n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def get_top_terms(model, feature_names, n_top_words, topic_idx):\n",
    "    max_topic = model.components_[topic_idx]\n",
    "    #relevant_terms = \" \".join([feature_names[i] for i in max_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    #print(\"Most Relevant Topic #%d:%s\" % (max_idx,relevant_terms))\n",
    "    relevant_terms = set()\n",
    "    for i in max_topic.argsort()[:-n_top_words - 1:-1]:\n",
    "        relevant_terms.add(feature_names[i])\n",
    "    return relevant_terms\n",
    "    \n",
    "#helper function to convert a set object to list that can be directly json encoded   \n",
    "def set_convert_to_json_list(terms_set):\n",
    "    json_term_list = list()\n",
    "    for term in terms_set:\n",
    "        json_term_list.append({\"name\":term})\n",
    "    return json_term_list\n",
    "\n",
    "#helper function to convert a dict object to list that can be directly json encoded\n",
    "def dict_convert_to_json_list(terms_dict):\n",
    "    json_term_list = list()\n",
    "    for key in terms_dict:\n",
    "        json_term_list.append({\"name\":key, \"children\":set_convert_to_json_list(terms_dict[key])})\n",
    "    return json_term_list\n",
    "    \n",
    "\n",
    "#exit if no user term is provided    \n",
    "if(len(sys.argv)!=2):\n",
    "    print (\"Invalid input...exit\")\n",
    "    exit()\n",
    "    \n",
    "user_term = sys.argv[1]\n",
    "\n",
    "#load the PMC dataset\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "\n",
    "data_samples = list()\n",
    "for folder, dirs, files in os.walk('../data/'):\n",
    "    for f in files:\n",
    "        if f.endswith('.nxml'):\n",
    "            tree = ET.parse(os.path.join(folder,f))\n",
    "            root = tree.getroot()\n",
    "            data_samples.append(' '.join(root.itertext())) #each document is represented by text content from that xml file\n",
    "            \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"%d lines read\" % len(data_samples))\n",
    "\n",
    "#use tf (raw term count) features for NMF.\n",
    "print(\"Extracting tfidf features for NMF...\")\n",
    "#max_df=0.95: when building the vocabulary ignore terms that have a document frequency strictly > 95% of documents\n",
    "#min_df=2: when building the vocabulary ignore terms that have a document frequency strictly < 2 \n",
    "#stop_words='english': a built-in stop word list for English is used\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "#fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features\")\n",
    "t0 = time()\n",
    "#alpha=.1: regularization\n",
    "#l1_ratio=.5: regularization\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#for debugging purposes\n",
    "#print(\"\\nFeature Names in NMF model:\")\n",
    "#print_feature_names(tf_feature_names)\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "print_top_words(nmf, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "#to extract terms(topics) that most commonly co-occurred with the user provided term (categorized in 2 levels)\n",
    "\n",
    "#set containing terms already shown in previous levels\n",
    "shown_terms = set()\n",
    "shown_terms.add(user_term)\n",
    "user_term_idx = find_term_idx(user_term,tf_feature_names)\n",
    "#l1_relevant_terms meaning the level 1 topics that are mostly commonly co-occurred\n",
    "l1_relevant_distribution_idx = find_most_relevant(nmf, user_term_idx, -1)\n",
    "l1_relevant_terms = get_top_terms(nmf, tf_feature_names, n_top_words, l1_relevant_distribution_idx)\n",
    "l1_relevant_terms = l1_relevant_terms.difference(shown_terms)\n",
    "#update shown_terms set for l2_relevant_terms\n",
    "shown_terms = shown_terms.union(l1_relevant_terms)\n",
    "l2_relevant_terms = dict()\n",
    "#level 2 topics are grouped by level 1 topics as keys\n",
    "for term in l1_relevant_terms:\n",
    "    term_idx = find_term_idx(term,tf_feature_names)\n",
    "    tmp_distribution_idx = find_most_relevant(nmf, term_idx, l1_relevant_distribution_idx)\n",
    "    l2_relevant_terms[term] = get_top_terms(nmf, tf_feature_names, n_top_words, tmp_distribution_idx)\n",
    "    l2_relevant_terms[term] = l2_relevant_terms[term].difference(shown_terms)\n",
    "    \n",
    "#convert obj to json file\n",
    "model = dict()\n",
    "model[\"name\"] = user_term\n",
    "model[\"children\"] = dict_convert_to_json_list(l2_relevant_terms)\n",
    "\n",
    "\n",
    "output = open('../output/flare_v4_10.json','w')\n",
    "output.write(json.dumps(model,indent=1)) # python will convert \\n to os.linesep\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/v4_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model visualization, n_distributions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
